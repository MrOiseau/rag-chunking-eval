"""
RAG Evaluation framework for assessing retrieval performance.
"""

import os
import json
from typing import List, Dict, Any, Generator, Tuple
from collections import defaultdict

from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from database.chromadb import ChromaDBHandler
from .models import Question
from .metrics import calculate_retrieval_metrics


class RagEvaluator:
    """
    Class for RAG evaluation using the Qasper dataset.
    Provides methods for loading questions, retrieving relevant documents,
    and calculating evaluation metrics.
    """

    def __init__(
        self,
        database: ChromaDBHandler,
        qasper_dir: str = "/app/data/dataset/qasper",
        similarity_threshold_pct: float = 0.3,  # 30% threshold
        use_llm: bool = True,
        temperature: float = 0.0,
        model: str = "gpt-4o-mini",
    ):
        """
        Initialize the RAG evaluator.

        Args:
            qasper_dir (str): Directory containing Qasper dataset
            database (ChromaDBHandler): ChromaDBHandler database
            similarity_threshold_pct (float): Threshold for similarity matching
            use_llm (bool): Whether to use LLM for answer generation
            temperature (float): Temperature for LLM generation
            model (str): Model to use for LLM generation
        """
        self.qasper_dir = qasper_dir

        # Database
        self.database = database

        # Metrics storage
        self.metrics = defaultdict(list)

        self.similarity_threshold_pct = similarity_threshold_pct

        # LLM settings
        self.use_llm = use_llm
        self.temperature = temperature
        self.model = model

        # Initialize LLM if needed
        if self.use_llm:
            self.llm = ChatOpenAI(model=self.model, temperature=self.temperature)

            # Define prompt template for answer generation
            self.answer_prompt_template = PromptTemplate(
                input_variables=["question", "context"],
                template="""
                You are a helpful assistant that answers questions based on the provided context.
                
                Context:
                {context}
                
                Question:
                {question}
                
                Answer the question based only on the provided context. If the context doesn't contain enough information to answer the question, say "I don't have enough information to answer this question."
                
                Answer:
                """,
            )

            # Define prompt template for answer evaluation
            self.evaluation_prompt_template = PromptTemplate(
                input_variables=[
                    "question",
                    "generated_answer",
                    "true_answer",
                    "evidence",
                ],
                template="""
                You are an expert evaluator of question answering systems. Your task is to evaluate the quality of an answer generated by an AI system.
                
                Question:
                {question}
                
                Generated Answer:
                {generated_answer}
                
                True Answer:
                {true_answer}
                
                Evidence:
                {evidence}
                
                Please evaluate the generated answer on a scale from 1 to 5, where:
                1 = Completely incorrect or irrelevant
                2 = Mostly incorrect but has some relevant information
                3 = Partially correct with some key information missing
                4 = Mostly correct with minor inaccuracies
                5 = Completely correct and comprehensive
                
                Consider the following criteria:
                - Factual correctness compared to the true answer
                - Completeness of the information
                - Relevance to the question
                - Use of the provided evidence
                
                Provide your rating as a single number between 1 and 5.
                
                Rating:
                """,
            )

    def extract_questions(self) -> Generator[List[Question], None, None]:
        """
        Extract questions from all JSON files in the Qasper dataset.
        Yields a list of Question objects for each JSON file processed.
        """
        # Get all subdirectories (each representing a paper)
        subdirs = [
            d
            for d in os.listdir(self.qasper_dir)
            if os.path.isdir(os.path.join(self.qasper_dir, d))
        ]

        for paper_dir in subdirs:
            paper_path = os.path.join(self.qasper_dir, paper_dir)

            # Look for the JSON file in the paper directory
            json_files = [f for f in os.listdir(paper_path) if f.endswith(".json")]

            for json_file in json_files:
                json_path = os.path.join(paper_path, json_file)

                try:
                    # Load the JSON file
                    with open(json_path, "r", encoding="utf-8") as f:
                        paper = json.load(f)

                    paper_id = paper.get(
                        "paper_id", os.path.basename(json_path).replace(".json", "")
                    )

                    questions: List[Question] = []
                    for record in paper["questions"]:
                        questions.append(
                            Question(
                                paper_id=paper_id,
                                question=record["question"],
                                free_form_answer=record["free_form_answer"],
                                evidence=record["evidence"],
                                highlighted_evidence=record["highlighted_evidence"],
                            )
                        )
                    # Yield the list of questions for this JSON file
                    yield questions

                except Exception as e:
                    print(f"Error processing file {json_path}: {e}")

    def generate_answer(
        self, question: str, paragraphs: List[Tuple[Document, float]]
    ) -> str:
        """
        Generate an answer to a question using retrieved paragraphs.

        Args:
            question (str): The question to answer
            paragraphs (List[Tuple[Document, float]]): Retrieved paragraphs with scores

        Returns:
            str: Generated answer
        """
        if not self.use_llm or not paragraphs:
            return "No answer generated (LLM disabled or no paragraphs retrieved)"

        # Extract text from paragraphs (only page_content, no scores)
        context_texts = [doc.page_content for doc, _ in paragraphs]

        # Join context texts
        context = "\n\n".join(context_texts)

        # Format the prompt content
        prompt_content = self.answer_prompt_template.format(
            question=question, context=context
        )

        # Generate answer using ChatOpenAI
        try:
            # Create a message with the prompt content
            messages = [{"role": "user", "content": prompt_content}]

            # Call the model with the messages
            response = self.llm.invoke(messages)

            # Extract the content from the response and ensure it's a string
            content = response.content
            if isinstance(content, str):
                return content
            else:
                # For any other type, convert to string
                return str(content)
        except Exception as e:
            print(f"Error generating answer: {e}")
            return f"Error generating answer: {str(e)}"

    def evaluate_answer_quality(
        self,
        question: str,
        generated_answer: str,
        true_answer: str,
        evidence: List[str],
    ) -> int:
        """
        Evaluate the quality of a generated answer compared to the ground truth.

        Args:
            question (str): The question text
            generated_answer (str): The answer generated by the LLM
            true_answer (str): The ground truth answer
            evidence (List[str]): The evidence passages

        Returns:
            int: Quality score from 1 to 5
        """
        if not self.use_llm:
            return 0  # No evaluation if LLM is disabled

        # Join evidence passages
        evidence_text = "\n\n".join([e for e in evidence if e])

        # Format the prompt content
        prompt_content = self.evaluation_prompt_template.format(
            question=question,
            generated_answer=generated_answer,
            true_answer=true_answer,
            evidence=evidence_text,
        )

        # Generate evaluation using ChatOpenAI
        try:
            # Create a message with the prompt content
            messages = [{"role": "user", "content": prompt_content}]

            # Call the model with the messages
            response = self.llm.invoke(messages)

            # Extract the content from the response and convert to int
            content = str(response.content)

            # Try to extract a number from the response
            try:
                # Look for a digit in the response
                import re

                match = re.search(r"(\d+)", content)
                if match:
                    score = int(match.group(1))
                    # Ensure score is between 1 and 5
                    return max(1, min(5, score))
                else:
                    print(f"Could not extract score from response: {content}")
                    return 0
            except Exception as e:
                print(f"Error extracting score: {e}")
                return 0

        except Exception as e:
            print(f"Error evaluating answer quality: {e}")
            return 0

    def evaluate_question(self, question: Question, k: int = 5) -> Dict[str, Any]:
        """
        Evaluate a single question using RAG and calculate metrics.

        Args:
            question (Question): The question to evaluate
            k (int): Number of paragraphs to retrieve

        Returns:
            Dict[str, Any]: Dictionary containing retrieved paragraphs and metrics
        """

        # Retrieve paragraphs using similarity search
        paragraphs = self.database.similarity_search_with_score(
            query=question.question, k=9, paper_id=question.paper_id
        )

        # Sort paragraphs by score in descending order (highest score first)
        paragraphs = sorted(paragraphs, key=lambda x: x[1], reverse=True)

        # Calculate metrics
        metrics = calculate_retrieval_metrics(
            question,
            paragraphs,
            k,
            similarity_threshold_pct=self.similarity_threshold_pct,
        )

        # Generate answer if LLM is enabled
        generated_answer = self.generate_answer(question.question, paragraphs[:k])

        # Evaluate answer quality
        answer_quality = self.evaluate_answer_quality(
            question=question.question,
            generated_answer=generated_answer,
            true_answer=question.free_form_answer,
            evidence=question.highlighted_evidence,
        )

        # Store metrics for this question
        for key, value in metrics.items():
            self.metrics[key].append(value)

        # Store answer quality metric
        self.metrics["answer_quality"].append(answer_quality)

        return {
            "question": question,
            "retrieved_paragraphs": paragraphs,
            "metrics": metrics,
            "generated_answer": generated_answer,
            "answer_quality": answer_quality,
        }

    def get_average_metrics(self) -> Dict[str, float]:
        """
        Get the average metrics across all evaluated questions.

        Returns:
            Dict[str, float]: Dictionary containing average precision, recall, and F1 scores
        """
        return {
            key: sum(values) / len(values) if values else 0
            for key, values in self.metrics.items()
        }

    def group_results_by_paper_id(
        self, results: List[Dict[str, Any]]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Group evaluation results by paper_id.

        Args:
            results (List[Dict[str, Any]]): List of evaluation results

        Returns:
            Dict[str, List[Dict[str, Any]]]: Dictionary mapping paper_id to list of results
        """
        grouped_results = defaultdict(list)

        for result in results:
            question = result["question"]
            paper_id = question.paper_id
            grouped_results[paper_id].append(result)

        return grouped_results

    def extract_serializable_data(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract serializable data from a result dictionary, including paragraphs and scores.

        Args:
            result (Dict[str, Any]): Result dictionary from evaluate_question

        Returns:
            Dict[str, Any]: Dictionary with serializable information
        """
        question = result["question"]
        paragraphs = result["retrieved_paragraphs"]

        # Extract information from paragraphs (Document objects can't be serialized directly)
        serializable_paragraphs = []
        for doc, score in paragraphs:
            serializable_paragraphs.append(
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": float(score),  # Ensure score is a native Python float
                }
            )

        # Get the generated answer and quality score if available
        generated_answer = result.get("generated_answer", "No answer generated")
        answer_quality = result.get("answer_quality", 0)

        return {
            "question_text": question.question,
            "paper_id": question.paper_id,
            "highlighted_evidence": question.highlighted_evidence,
            "free_form_answer": question.free_form_answer,
            "generated_answer": generated_answer,
            "answer_quality": answer_quality,
            "metrics": result["metrics"],
            "retrieved_paragraphs": serializable_paragraphs,
        }

    def calculate_paper_metrics(
        self, paper_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Calculate aggregate metrics for a specific paper.

        Args:
            paper_results (List[Dict[str, Any]]): List of evaluation results for a paper

        Returns:
            Dict[str, Any]: Dictionary containing aggregate metrics for the paper
        """
        # Extract metrics from each result
        precision_values = [r["metrics"]["precision"] for r in paper_results]
        recall_values = [r["metrics"]["recall"] for r in paper_results]
        f1_values = [r["metrics"]["f1"] for r in paper_results]
        answer_quality_values = [r.get("answer_quality", 0) for r in paper_results]

        # Extract MRR metrics if available
        mrr_at_3_values = [
            r["metrics"].get("mrr@3", 0)
            for r in paper_results
            if "mrr@3" in r["metrics"]
        ]
        mrr_at_5_values = [
            r["metrics"].get("mrr@5", 0)
            for r in paper_results
            if "mrr@5" in r["metrics"]
        ]
        mrr_at_7_values = [
            r["metrics"].get("mrr@7", 0)
            for r in paper_results
            if "mrr@7" in r["metrics"]
        ]
        mrr_at_9_values = [
            r["metrics"].get("mrr@9", 0)
            for r in paper_results
            if "mrr@9" in r["metrics"]
        ]

        # Extract R@k metrics if available
        r_at_3_values = [
            r["metrics"].get("r@3", 0) for r in paper_results if "r@3" in r["metrics"]
        ]
        r_at_5_values = [
            r["metrics"].get("r@5", 0) for r in paper_results if "r@5" in r["metrics"]
        ]
        r_at_7_values = [
            r["metrics"].get("r@7", 0) for r in paper_results if "r@7" in r["metrics"]
        ]
        r_at_9_values = [
            r["metrics"].get("r@9", 0) for r in paper_results if "r@9" in r["metrics"]
        ]

        # Calculate averages
        avg_precision = (
            sum(precision_values) / len(precision_values) if precision_values else 0
        )
        avg_recall = sum(recall_values) / len(recall_values) if recall_values else 0
        avg_f1 = sum(f1_values) / len(f1_values) if f1_values else 0
        avg_answer_quality = (
            sum(answer_quality_values) / len(answer_quality_values)
            if answer_quality_values
            else 0
        )

        # Calculate MRR averages
        avg_mrr_at_3 = (
            sum(mrr_at_3_values) / len(mrr_at_3_values) if mrr_at_3_values else 0
        )
        avg_mrr_at_5 = (
            sum(mrr_at_5_values) / len(mrr_at_5_values) if mrr_at_5_values else 0
        )
        avg_mrr_at_7 = (
            sum(mrr_at_7_values) / len(mrr_at_7_values) if mrr_at_7_values else 0
        )
        avg_mrr_at_9 = (
            sum(mrr_at_9_values) / len(mrr_at_9_values) if mrr_at_9_values else 0
        )

        # Calculate R@k averages
        avg_r_at_3 = sum(r_at_3_values) / len(r_at_3_values) if r_at_3_values else 0
        avg_r_at_5 = sum(r_at_5_values) / len(r_at_5_values) if r_at_5_values else 0
        avg_r_at_7 = sum(r_at_7_values) / len(r_at_7_values) if r_at_7_values else 0
        avg_r_at_9 = sum(r_at_9_values) / len(r_at_9_values) if r_at_9_values else 0

        # Extract serializable data from each result
        question_data = [self.extract_serializable_data(r) for r in paper_results]

        result = {
            "average_precision": avg_precision,
            "average_recall": avg_recall,
            "average_f1": avg_f1,
            "average_answer_quality": avg_answer_quality,
            "question_count": len(paper_results),
            "questions": question_data,
        }

        # Add MRR metrics if available
        if mrr_at_3_values:
            result["average_mrr@3"] = avg_mrr_at_3
        if mrr_at_5_values:
            result["average_mrr@5"] = avg_mrr_at_5
        if mrr_at_7_values:
            result["average_mrr@7"] = avg_mrr_at_7
        if mrr_at_9_values:
            result["average_mrr@9"] = avg_mrr_at_9

        # Add R@k metrics if available
        if r_at_3_values:
            result["average_r@3"] = avg_r_at_3
        if r_at_5_values:
            result["average_r@5"] = avg_r_at_5
        if r_at_7_values:
            result["average_r@7"] = avg_r_at_7
        if r_at_9_values:
            result["average_r@9"] = avg_r_at_9

        return result

    def save_paper_metrics(
        self,
        paper_id: str,
        metrics: Dict[str, Any],
        output_dir: str = "/app/data/evaluation_results",
    ) -> str:
        """
        Save metrics for a specific paper to a JSON file.

        Args:
            paper_id (str): ID of the paper
            metrics (Dict[str, Any]): Metrics to save
            output_dir (str): Directory to save the metrics file

        Returns:
            str: Path to the saved metrics file
        """
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)

        # Create filename
        filename = f"{paper_id}_metrics.json"
        filepath = os.path.join(output_dir, filename)

        # Save metrics to file
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(metrics, f, indent=2)

        return filepath

    def evaluate_dataset(
        self,
        k: int = 5,
        save_metrics: bool = True,
        output_dir: str = "/app/data/evaluation_results",
    ) -> Dict[str, Any]:
        """
        Evaluate the entire dataset and return aggregated results.

        Args:
            k (int): Number of paragraphs to retrieve for each question
            save_metrics (bool): Whether to save metrics for each paper_id
            output_dir (str): Directory to save metrics files

        Returns:
            Dict[str, Any]: Dictionary containing all results and average metrics
        """
        results = []

        # Process all questions
        for questions_list in self.extract_questions():
            for question in questions_list:
                result = self.evaluate_question(question, k=k)
                results.append(result)

        # Group results by paper_id
        grouped_results = self.group_results_by_paper_id(results)

        # Calculate and save metrics for each paper
        paper_metrics = {}
        saved_files = {}

        for paper_id, paper_results in grouped_results.items():
            # Calculate metrics for this paper
            metrics = self.calculate_paper_metrics(paper_results)
            paper_metrics[paper_id] = metrics

            # Save metrics to file if requested
            if save_metrics:
                filepath = self.save_paper_metrics(paper_id, metrics, output_dir)
                saved_files[paper_id] = filepath
                print(f"Saved metrics for paper {paper_id} to {filepath}")

        # Calculate average metrics across all questions
        avg_metrics = self.get_average_metrics()

        return {
            "results": results,
            "grouped_results": grouped_results,
            "paper_metrics": paper_metrics,
            "average_metrics": avg_metrics,
            "total_questions": len(results),
            "saved_files": saved_files if save_metrics else None,
        }

    def print_evaluation_summary(self, results: Dict[str, Any]) -> None:
        """
        Print a summary of evaluation results.

        Args:
            results (Dict[str, Any]): Results from evaluate_dataset
        """
        avg_metrics = results["average_metrics"]
        paper_metrics = results.get("paper_metrics", {})

        print("\n===== RAG Evaluation Summary =====")
        print(f"Total questions evaluated: {results['total_questions']}")
        print(f"Total papers evaluated: {len(paper_metrics)}")
        print(f"Average Precision: {avg_metrics['precision']:.4f}")
        print(f"Average Recall: {avg_metrics['recall']:.4f}")
        print(f"Average F1 Score: {avg_metrics['f1']:.4f}")

        # Print MRR metrics
        if "mrr@3" in avg_metrics:
            print(f"Average MRR@3: {avg_metrics['mrr@3']:.4f}")
        if "mrr@5" in avg_metrics:
            print(f"Average MRR@5: {avg_metrics['mrr@5']:.4f}")
        if "mrr@7" in avg_metrics:
            print(f"Average MRR@7: {avg_metrics['mrr@7']:.4f}")
        if "mrr@9" in avg_metrics:
            print(f"Average MRR@9: {avg_metrics['mrr@9']:.4f}")

        # Print R@k metrics
        if "r@3" in avg_metrics:
            print(f"Average R@3: {avg_metrics['r@3']:.4f}")
        if "r@5" in avg_metrics:
            print(f"Average R@5: {avg_metrics['r@5']:.4f}")
        if "r@7" in avg_metrics:
            print(f"Average R@7: {avg_metrics['r@7']:.4f}")
        if "r@9" in avg_metrics:
            print(f"Average R@9: {avg_metrics['r@9']:.4f}")

        print(
            f"Average Answer Quality: {avg_metrics.get('answer_quality', 0):.2f} / 5.00"
        )

        # Print per-paper summary
        if paper_metrics:
            print("\n----- Per-Paper Metrics -----")
            for paper_id, metrics in paper_metrics.items():
                print(f"\nPaper ID: {paper_id}")
                print(f"  Questions: {metrics['question_count']}")
                print(f"  Precision: {metrics['average_precision']:.4f}")
                print(f"  Recall: {metrics['average_recall']:.4f}")
                print(f"  F1 Score: {metrics['average_f1']:.4f}")

                # Print MRR metrics if available
                if "average_mrr@3" in metrics:
                    print(f"  MRR@3: {metrics['average_mrr@3']:.4f}")
                if "average_mrr@5" in metrics:
                    print(f"  MRR@5: {metrics['average_mrr@5']:.4f}")
                if "average_mrr@7" in metrics:
                    print(f"  MRR@7: {metrics['average_mrr@7']:.4f}")
                if "average_mrr@9" in metrics:
                    print(f"  MRR@9: {metrics['average_mrr@9']:.4f}")

                # Print R@k metrics if available
                if "average_r@3" in metrics:
                    print(f"  R@3: {metrics['average_r@3']:.4f}")
                if "average_r@5" in metrics:
                    print(f"  R@5: {metrics['average_r@5']:.4f}")
                if "average_r@7" in metrics:
                    print(f"  R@7: {metrics['average_r@7']:.4f}")
                if "average_r@9" in metrics:
                    print(f"  R@9: {metrics['average_r@9']:.4f}")

                print(
                    f"  Answer Quality: {metrics.get('average_answer_quality', 0):.2f} / 5.00"
                )
