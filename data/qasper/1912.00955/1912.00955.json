{
  "paper_id": "1912.00955",
  "title": "Dynamic Prosody Generation for Speech Synthesis using Linguistics-Driven Acoustic Embedding Selection",
  "questions": [
    {
      "question": "What dataset is used for train/test of this method?",
      "free_form_answer": "Training datasets: TTS System dataset and embedding selection dataset. Evaluation datasets: Common Prosody Errors dataset and LFR dataset.",
      "evidence": [
        "Experimental Protocol ::: Datasets ::: Training Dataset",
        "(i) TTS System dataset: We trained our TTS system with a mixture of neutral and newscaster style speech. For a total of 24 hours of training data, split in 20 hours of neutral (22000 utterances) and 4 hours of newscaster styled speech (3000 utterances).",
        "(ii) Embedding selection dataset: As the evaluation was carried out only on the newscaster speaking style, we restrict our linguistic search space to the utterances associated to the newscaster style: 3000 sentences.",
        "Experimental Protocol ::: Datasets ::: Evaluation Dataset",
        "The systems were evaluated on two datasets:",
        "(i) Common Prosody Errors (CPE): The dataset on which the baseline Prostron model fails to generate appropriate prosody. This dataset consists of complex utterances like compound nouns (22%), \u201cor\" questions (9%), \u201cwh\" questions (18%). This set is further enhanced by sourcing complex utterances (51%) from BIBREF24.",
        "(ii) LFR: As demonstrated in BIBREF25, evaluating sentences in isolation does not suffice if we want to evaluate the quality of long-form speech. Thus, for evaluations on LFR we curated a dataset of news samples. The news style sentences were concatenated into full news stories, to capture the overall experience of our intended use case."
      ],
      "highlighted_evidence": [
        "Experimental Protocol ::: Datasets ::: Training Dataset\n(i) TTS System dataset: We trained our TTS system with a mixture of neutral and newscaster style speech. For a total of 24 hours of training data, split in 20 hours of neutral (22000 utterances) and 4 hours of newscaster styled speech (3000 utterances).\n\n(ii) Embedding selection dataset: As the evaluation was carried out only on the newscaster speaking style, we restrict our linguistic search space to the utterances associated to the newscaster style: 3000 sentences.\n\nExperimental Protocol ::: Datasets ::: Evaluation Dataset\nThe systems were evaluated on two datasets:\n\n(i) Common Prosody Errors (CPE): The dataset on which the baseline Prostron model fails to generate appropriate prosody. This dataset consists of complex utterances like compound nouns (22%), \u201cor\" questions (9%), \u201cwh\" questions (18%). This set is further enhanced by sourcing complex utterances (51%) from BIBREF24.\n\n(ii) LFR: As demonstrated in BIBREF25, evaluating sentences in isolation does not suffice if we want to evaluate the quality of long-form speech. Thus, for evaluations on LFR we curated a dataset of news samples. The news style sentences were concatenated into full news stories, to capture the overall experience of our intended use case."
      ]
    }
  ]
}