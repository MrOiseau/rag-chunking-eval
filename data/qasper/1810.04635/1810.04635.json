{
  "paper_id": "1810.04635",
  "title": "Multimodal Speech Emotion Recognition Using Audio and Text",
  "questions": [
    {
      "question": "Do they use datasets with transcribed text or do they determine text from the audio?",
      "free_form_answer": "They use text transcription.",
      "evidence": [
        "We assume that speech transcripts can be extracted from audio signals with high accuracy, given the advancement of ASR technologies BIBREF7 . We attempt to use the processed textual information as another modality in predicting the emotion class of a given signal. To use textual information, a speech transcript is tokenized and indexed into a sequence of tokens using the Natural Language Toolkit (NLTK) BIBREF27 . Each token is then passed through a word-embedding layer that converts a word index to a corresponding 300-dimensional vector that contains additional contextual meaning between words. The sequence of embedded tokens is fed into a text recurrent encoder (TRE) in such a way that the audio MFCC features are encoded using the ARE represented by equation EQREF2 . In this case, INLINEFORM0 is the t- INLINEFORM1 embedded token from the text input. Finally, the emotion class is predicted from the last hidden state of the text-RNN using the softmax function."
      ],
      "highlighted_evidence": [
        "We assume that speech transcripts can be extracted from audio signals with high accuracy, given the advancement of ASR technologies BIBREF7 ."
      ]
    },
    {
      "question": "Do they use datasets with transcribed text or do they determine text from the audio?",
      "free_form_answer": "both",
      "evidence": [
        "To overcome these limitations, we propose a model that uses high-level text transcription, as well as low-level audio signals, to utilize the information contained within low-resource datasets to a greater degree. Given recent improvements in automatic speech recognition (ASR) technology BIBREF7 , BIBREF2 , BIBREF8 , BIBREF9 , speech transcription can be carried out using audio signals with considerable skill. The emotional content of speech is clearly indicated by the emotion words contained in a sentence BIBREF10 , such as \u201clovely\u201d and \u201cawesome,\u201d which carry strong emotions compared to generic (non-emotion) words, such as \u201cperson\u201d and \u201cday.\u201d Thus, we hypothesize that the speech emotion recognition model will be benefit from the incorporation of high-level textual input."
      ],
      "highlighted_evidence": [
        "To overcome these limitations, we propose a model that uses high-level text transcription, as well as low-level audio signals, to utilize the information contained within low-resource datasets to a greater degree."
      ]
    }
  ]
}