{
  "paper_id": "1906.00346",
  "title": "Pre-training of Graph Augmented Transformers for Medication Recommendation",
  "questions": [
    {
      "question": "Is the G-BERT model useful beyond the task considered?",
      "free_form_answer": "There is nothing specific about the approach that depends on medical recommendations. The approach combines graph data and text data into a single embedding.",
      "evidence": [
        "FLOAT SELECTED: Figure 2: The framework of G-BERT. It consists of three main parts: ontology embedding, BERT and fine-tuned classifier. Firstly, we derive ontology embedding for medical code laid in leaf nodes by cooperating ancestors information by Eq. 1 and 2 based on graph attention networks (Eq. 3, 4). Then we input set of diagnosis and medication ontology embedding separately to shared weight BERT which is pretrained using Eq. 6, 7, 8. Finally, we concatenate the mean of all previous visit embeddings and the last visit embedding as input and fine-tune the prediction layers using Eq. 10 for medication recommendation tasks."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Figure 2: The framework of G-BERT. It consists of three main parts: ontology embedding, BERT and fine-tuned classifier. Firstly, we derive ontology embedding for medical code laid in leaf nodes by cooperating ancestors information by Eq. 1 and 2 based on graph attention networks (Eq. 3, 4). Then we input set of diagnosis and medication ontology embedding separately to shared weight BERT which is pretrained using Eq. 6, 7, 8. Finally, we concatenate the mean of all previous visit embeddings and the last visit embedding as input and fine-tune the prediction layers using Eq. 10 for medication recommendation tasks."
      ]
    },
    {
      "question": "Is the G-BERT model useful beyond the task considered?",
      "free_form_answer": "It learns a representation of medical records. The learned representation (embeddings) can be used for other predictive tasks involving information from electronic health records.",
      "evidence": [
        "In this paper we proposed a pre-training model named G-BERT for medical code representation and medication recommendation. To our best knowledge, G-BERT is the first that utilizes language model pre-training techniques in healthcare domain. It adapted BERT to the EHR data and integrated medical ontology information using graph neural networks. By additional pre-training on the EHR from patients who only have one hospital visit which are generally discarded before model training, G-BERT outperforms all baselines in prediction accuracy on medication recommendation task. One direction for the future work is to add more auxiliary and structural tasks to improve the ability of code representaion. Another direction may be to adapt our model to be suitable for even larger datasets with more heterogeneous modalities."
      ],
      "highlighted_evidence": [
        "In this paper we proposed a pre-training model named G-BERT for medical code representation and medication recommendation."
      ]
    }
  ]
}